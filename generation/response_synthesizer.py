"""
Response synthesis for DocuLens.

This module provides response generation using GPT-5-nano with
proper citation handling and context formatting.
"""

import re
from dataclasses import dataclass, field
from typing import List, Optional, Dict, Any, Set

from openai import OpenAI
from llama_index.core.schema import NodeWithScore

from config.settings import settings
from config.logging_config import get_logger, log_execution_time, LogTimer
from generation.prompts import (
    get_system_prompt,
    format_context_prompt,
    get_no_context_response,
    REFINE_PROMPT,
)

logger = get_logger(__name__)


# Default LLM settings
DEFAULT_LLM_MODEL = "gpt-5-nano-2025-08-07"
DEFAULT_TEMPERATURE = 0.1
DEFAULT_MAX_TOKENS = 1024

# Citation pattern for validation
CITATION_PATTERN = re.compile(r'\[Page\s+(\d+)(?:,\s*(?:Image:\s*[\w\-\.]+|Table))?\]')


@dataclass
class SynthesizedResponse:
    """
    Response generated by the synthesizer.
    
    Attributes:
        query: The original query.
        response: The generated response text.
        citations: List of extracted citations from the response.
        cited_pages: Set of page numbers cited.
        context_pages: Set of page numbers in the provided context.
        sources: Source information from retrieved nodes.
        metadata: Additional metadata about generation.
    """
    query: str
    response: str
    citations: List[str] = field(default_factory=list)
    cited_pages: Set[int] = field(default_factory=set)
    context_pages: Set[int] = field(default_factory=set)
    sources: List[Dict[str, Any]] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    @property
    def has_valid_citations(self) -> bool:
        """Check if all cited pages are in the context."""
        return self.cited_pages.issubset(self.context_pages)
    
    @property
    def uncited_pages(self) -> Set[int]:
        """Get pages in context that weren't cited."""
        return self.context_pages - self.cited_pages
    
    @property
    def invalid_citations(self) -> Set[int]:
        """Get cited pages that aren't in context (hallucinated)."""
        return self.cited_pages - self.context_pages


class ResponseSynthesizer:
    """
    Generates responses with citations using GPT-5-nano.
    
    This class handles:
    - Formatting retrieved context for the LLM
    - Generating responses with proper citations
    - Validating citations against source context
    """
    
    def __init__(
        self,
        model: str = DEFAULT_LLM_MODEL,
        temperature: float = DEFAULT_TEMPERATURE,
        max_tokens: int = DEFAULT_MAX_TOKENS,
        api_key: Optional[str] = None,
    ):
        """
        Initialize the response synthesizer.
        
        Args:
            model: OpenAI model to use for generation.
            temperature: Sampling temperature (lower = more deterministic).
            max_tokens: Maximum tokens in the response.
            api_key: OpenAI API key. Defaults to settings.
        """
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        
        logger.info(f"Initializing ResponseSynthesizer (model={model}, temp={temperature})")
        
        # Initialize OpenAI client
        self.client = OpenAI(api_key=api_key or settings.OPENAI_API_KEY)
    
    def format_context(self, nodes: List[NodeWithScore]) -> str:
        """
        Format retrieved nodes into context string.
        
        Args:
            nodes: List of retrieved nodes with scores.
            
        Returns:
            Formatted context string with page citations.
        """
        if not nodes:
            logger.debug("No nodes to format")
            return ""
        
        logger.debug(f"Formatting context from {len(nodes)} nodes")
        context_parts = []
        
        for node_with_score in nodes:
            node = node_with_score.node
            metadata = node.metadata
            page_num = metadata.get("page_number", 1)
            content_type = metadata.get("content_type", "text")
            
            # Format source indicator
            if content_type == "image_summary":
                image_name = metadata.get("image_name", "image")
                prefix = f"[Page {page_num}, Image: {image_name}]"
            elif content_type == "table":
                prefix = f"[Page {page_num}, Table]"
            else:
                prefix = f"[Page {page_num}]"
            
            # Get node content
            node_text = node.get_content()
            context_parts.append(f"{prefix}\n{node_text}")
        
        return "\n\n---\n\n".join(context_parts)
    
    def get_context_pages(self, nodes: List[NodeWithScore]) -> Set[int]:
        """Extract all page numbers from retrieved nodes."""
        pages = set()
        for node_with_score in nodes:
            page = node_with_score.node.metadata.get("page_number", 1)
            pages.add(page)
        return pages
    
    def extract_citations(self, response: str) -> tuple[List[str], Set[int]]:
        """
        Extract citations from a response.
        
        Args:
            response: Generated response text.
            
        Returns:
            Tuple of (list of citation strings, set of page numbers).
        """
        citations = CITATION_PATTERN.findall(response)
        full_citations = CITATION_PATTERN.findall(response)
        
        # Extract just page numbers
        page_numbers = set()
        for match in re.finditer(CITATION_PATTERN, response):
            page_num = int(match.group(1))
            page_numbers.add(page_num)
        
        # Get full citation strings
        citation_strings = [match.group(0) for match in re.finditer(CITATION_PATTERN, response)]
        
        return citation_strings, page_numbers
    
    def get_source_info(self, nodes: List[NodeWithScore]) -> List[Dict[str, Any]]:
        """Extract source information from nodes."""
        sources = []
        for node_with_score in nodes:
            node = node_with_score.node
            metadata = node.metadata
            node_text = node.get_content()
            
            source = {
                "page_number": metadata.get("page_number", 1),
                "content_type": metadata.get("content_type", "text"),
                "source_document": metadata.get("source_document", "unknown"),
                "score": node_with_score.score,
                "text_preview": node_text[:200] + "..." if len(node_text) > 200 else node_text,
            }
            
            if metadata.get("content_type") == "image_summary":
                source["image_name"] = metadata.get("image_name", "")
            
            sources.append(source)
        
        return sources
    
    def has_multimodal_content(self, nodes: List[NodeWithScore]) -> bool:
        """Check if nodes contain image summaries."""
        for node_with_score in nodes:
            if node_with_score.node.metadata.get("content_type") == "image_summary":
                return True
        return False
    
    @log_execution_time()
    def synthesize(
        self,
        query: str,
        nodes: List[NodeWithScore],
        system_prompt: Optional[str] = None,
    ) -> SynthesizedResponse:
        """
        Generate a response for a query using retrieved context.
        
        Args:
            query: User's question.
            nodes: Retrieved nodes with relevance scores.
            system_prompt: Optional custom system prompt.
            
        Returns:
            SynthesizedResponse with generated answer and metadata.
        """
        logger.info(f"Synthesizing response for query: '{query[:50]}...'" if len(query) > 50 else f"Synthesizing response for query: '{query}'")
        
        # Handle empty context
        if not nodes:
            logger.warning("No context nodes provided, returning no-context response")
            return SynthesizedResponse(
                query=query,
                response=get_no_context_response(),
                metadata={"model": self.model, "no_context": True},
            )
        
        logger.debug(f"Generating response with {len(nodes)} context nodes")
        
        # Format context
        context = self.format_context(nodes)
        context_pages = self.get_context_pages(nodes)
        
        # Determine system prompt
        if system_prompt is None:
            multimodal = self.has_multimodal_content(nodes)
            system_prompt = get_system_prompt(multimodal=multimodal)
        
        # Format user prompt
        user_prompt = format_context_prompt(context, query)
        
        # Generate response
        logger.debug(f"Calling {self.model} API...")
        with LogTimer(logger, f"LLM generation ({self.model})"):
            completion = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=self.temperature,
                max_tokens=self.max_tokens,
            )
        
        response_text = completion.choices[0].message.content or ""
        
        # Extract citations
        citations, cited_pages = self.extract_citations(response_text)
        logger.debug(f"Response contains {len(citations)} citations from pages: {cited_pages}")
        
        # Get source info
        sources = self.get_source_info(nodes)
        
        # Log token usage
        if completion.usage:
            logger.debug(f"Token usage: prompt={completion.usage.prompt_tokens}, completion={completion.usage.completion_tokens}, total={completion.usage.total_tokens}")
        
        logger.info(f"Response synthesized successfully ({len(response_text)} chars)")
        
        return SynthesizedResponse(
            query=query,
            response=response_text,
            citations=citations,
            cited_pages=cited_pages,
            context_pages=context_pages,
            sources=sources,
            metadata={
                "model": self.model,
                "temperature": self.temperature,
                "usage": {
                    "prompt_tokens": completion.usage.prompt_tokens if completion.usage else 0,
                    "completion_tokens": completion.usage.completion_tokens if completion.usage else 0,
                    "total_tokens": completion.usage.total_tokens if completion.usage else 0,
                },
            },
        )
    
    def synthesize_streaming(
        self,
        query: str,
        nodes: List[NodeWithScore],
        system_prompt: Optional[str] = None,
    ):
        """
        Generate a streaming response for a query using retrieved context.
        
        Yields chunks of the response as they are generated.
        
        Args:
            query: User's question.
            nodes: Retrieved nodes with relevance scores.
            system_prompt: Optional custom system prompt.
        """
        logger.info(f"Synthesizing streaming response for query: '{query[:50]}...'" if len(query) > 50 else f"Synthesizing streaming response for query: '{query}'")
        
        # Handle empty context
        if not nodes:
            logger.warning("No context nodes provided, returning no-context response")
            yield get_no_context_response()
            return
        
        logger.debug(f"Generating streaming response with {len(nodes)} context nodes")
        
        # Format context
        context = self.format_context(nodes)
        
        # Determine system prompt
        if system_prompt is None:
            multimodal = self.has_multimodal_content(nodes)
            system_prompt = get_system_prompt(multimodal=multimodal)
        
        # Format user prompt
        user_prompt = format_context_prompt(context, query)
        
        # Generate streaming response
        logger.debug(f"Calling {self.model} API with streaming...")
        try:
            stream = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                stream=True,
            )
            
            for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
        
        except Exception as e:
            logger.error(f"Streaming failed: {e}")
            yield f"Error: {str(e)}"
    
    def synthesize_with_refinement(
        self,
        query: str,
        nodes: List[NodeWithScore],
        initial_response: Optional[str] = None,
    ) -> SynthesizedResponse:
        """
        Generate a response with optional refinement pass.
        
        If an initial response is provided, it will be refined with
        the new context. Otherwise, generates a fresh response.
        
        Args:
            query: User's question.
            nodes: Retrieved nodes.
            initial_response: Optional previous response to refine.
            
        Returns:
            SynthesizedResponse (refined if initial_response provided).
        """
        if initial_response is None:
            return self.synthesize(query, nodes)
        
        # Format new context
        context = self.format_context(nodes)
        
        # Use refinement prompt
        refine_prompt = REFINE_PROMPT.format(
            previous_response=initial_response,
            new_context=context,
        )
        
        completion = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": get_system_prompt()},
                {"role": "user", "content": refine_prompt},
            ],
            temperature=self.temperature,
            max_tokens=self.max_tokens,
        )
        
        response_text = completion.choices[0].message.content or ""
        citations, cited_pages = self.extract_citations(response_text)
        context_pages = self.get_context_pages(nodes)
        
        return SynthesizedResponse(
            query=query,
            response=response_text,
            citations=citations,
            cited_pages=cited_pages,
            context_pages=context_pages,
            sources=self.get_source_info(nodes),
            metadata={"model": self.model, "refined": True},
        )


def create_response_synthesizer(
    model: str = DEFAULT_LLM_MODEL,
    temperature: float = DEFAULT_TEMPERATURE,
    max_tokens: int = DEFAULT_MAX_TOKENS,
) -> ResponseSynthesizer:
    """
    Factory function to create a ResponseSynthesizer.
    
    Args:
        model: OpenAI model to use.
        temperature: Sampling temperature.
        max_tokens: Maximum tokens in response.
        
    Returns:
        Configured ResponseSynthesizer instance.
    """
    logger.info(f"Creating ResponseSynthesizer (model={model})")
    return ResponseSynthesizer(
        model=model,
        temperature=temperature,
        max_tokens=max_tokens,
    )
